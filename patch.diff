Submodule server_side/scripts/image_processing/llava-cpp-server contains modified content
diff --git a/server_side/scripts/image_processing/llava-cpp-server/llava_server.cpp b/server_side/scripts/image_processing/llava-cpp-server/llava_server.cpp
index fe13c7e..9c3552f 100644
--- a/server_side/scripts/image_processing/llava-cpp-server/llava_server.cpp
+++ b/server_side/scripts/image_processing/llava-cpp-server/llava_server.cpp
@@ -236,7 +236,7 @@ int main(int argc, char ** argv)
     gpt_params params;
 
     std::string hostname = "localhost";
-    int port = 8080;
+    int port = 6000;
     bool enable_http_logging = false;
     if (!parse_command_line(argc, argv, params, hostname, port, enable_http_logging))
     {
@@ -265,25 +265,24 @@ int main(int argc, char ** argv)
         return 1;
     }
 
-    llama_context_params ctx_params = llama_context_default_params();
-
-    ctx_params.n_ctx           = params.n_ctx < 2048 ? 2048 : params.n_ctx; // we need a longer context size to process image embeddings
-    ctx_params.n_threads       = params.n_threads;
-    ctx_params.n_threads_batch = params.n_threads_batch == -1 ? params.n_threads : params.n_threads_batch;
-
-    // create a llama context once that we'll reuse for each request
-    llama_context * ctx_llama = llama_new_context_with_model(model, ctx_params);
-    if (ctx_llama == NULL)
-    {
-        fprintf(stderr , "%s: error: failed to create the llama_context\n" , __func__);
-        return 1;
-    }
-
     // Serve forever
     std::mutex mtx;
     run_web_server(hostname, port, enable_http_logging,
-        [&mtx, &params, ctx_clip, ctx_llama](const llava_request &request, httplib::Response &response)
+        [&mtx, &params, &model, ctx_clip](const llava_request &request, httplib::Response &response)
         {
+			llama_context_params ctx_params = llama_context_default_params();
+
+			ctx_params.n_ctx           = params.n_ctx < 2048 ? 2048 : params.n_ctx; // we need a longer context size to process image embeddings
+			ctx_params.n_threads       = params.n_threads;
+			ctx_params.n_threads_batch = params.n_threads_batch == -1 ? params.n_threads : params.n_threads_batch;
+
+			llama_context * ctx_llama = llama_new_context_with_model(model, ctx_params);
+			if (ctx_llama == NULL)
+			{
+				fprintf(stderr , "%s: error: failed to create the llama_context\n" , __func__);
+				return;
+			}
+
             std::unique_lock lock(mtx);
             perform_inference(request, response, params, ctx_clip, ctx_llama);
         }
